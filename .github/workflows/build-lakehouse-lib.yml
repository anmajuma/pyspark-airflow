
name: Pyspark LakeHouse Lib Build 
on: workflow_dispatch
jobs:
  whl-builder:
    runs-on: self-hosted
    steps:
    # - uses: actions/checkout@v1
    # - name: Set up Python 3.8
    #   uses: actions/setup-python@v1
    #   with:
    #     python-version: 3.8
    - name: Install Requirements
      run: |
        python -m pip install --upgrade pip
        pip install setuptools
        pip install wheel        
    - name: Build Python Package
      run: |
        python ./spark/setup.py sdist bdist_wheel
        mkdir ./whlpkg
        cp -r ./dist/ ./whlpkg        
    - name: Temporarily save Whl Package
      uses: actions/upload-artifact@v4
      with:
          name: custom-python-package-artifact
          path: ./whlpkg
          retention-days: 1
          overwrite: true
  whl-uploader:
    runs-on: self-hosted
    needs: whl-builder
    steps:
    - uses: actions/download-artifact@v4
      with:
        name: custom-python-package-artifact
    - name: Display structure of downloaded files
      run: ls -R
    # - name: Configure AWS Credentials
    #   uses: aws-actions/configure-aws-credentials@v4
    #   with:
    #     aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
    #     aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
    #     aws-region: us-east-1
    # - name: Upload Libs to S3 
    #   run: |
    #       pip install -U awscli
    #       aws s3 cp ./dist s3://aws-db-s3-bkt/whls/ --recursive
    - name: Copy Whls to Spark Cluster 
      run: |
        docker cp ./whlpkg/. airflow-spark-spark-1:/opt/bitnami/spark
        docker cp ./whlpkg/. airflow-spark-spark-worker-1:/opt/bitnami/spark
        docker cp ./whlpkg/. airflow-spark-spark-worker-2:/opt/bitnami/spark
        docker cp ./whlpkg/. airflow-spark-spark-worker-3:/opt/bitnami/spark
    - name: Install Whls to Spark Cluster 
      run: |
        
        docker exec --user root airflow-spark-spark-1 python -m pip install /opt/bitnami/spark/dist/lakehouse-0.1-py3-none-any.whl     
        docker exec --user root airflow-spark-spark-worker-1 python -m pip install /opt/bitnami/spark/dist/lakehouse-0.1-py3-none-any.whl     
        docker exec --user root airflow-spark-spark-worker-2 python -m pip install /opt/bitnami/spark/dist/lakehouse-0.1-py3-none-any.whl     
        docker exec --user root airflow-spark-spark-worker-3 python -m pip install /opt/bitnami/spark/dist/lakehouse-0.1-py3-none-any.whl     
