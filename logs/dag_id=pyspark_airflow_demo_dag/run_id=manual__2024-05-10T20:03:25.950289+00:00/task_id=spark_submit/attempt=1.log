[2024-05-10T20:03:27.946+0000] {taskinstance.py:1171} INFO - Dependencies all met for <TaskInstance: pyspark_airflow_demo_dag.spark_submit manual__2024-05-10T20:03:25.950289+00:00 [queued]>
[2024-05-10T20:03:27.957+0000] {taskinstance.py:1171} INFO - Dependencies all met for <TaskInstance: pyspark_airflow_demo_dag.spark_submit manual__2024-05-10T20:03:25.950289+00:00 [queued]>
[2024-05-10T20:03:27.957+0000] {taskinstance.py:1368} INFO - 
--------------------------------------------------------------------------------
[2024-05-10T20:03:27.958+0000] {taskinstance.py:1369} INFO - Starting attempt 1 of 1
[2024-05-10T20:03:27.958+0000] {taskinstance.py:1370} INFO - 
--------------------------------------------------------------------------------
[2024-05-10T20:03:27.973+0000] {taskinstance.py:1389} INFO - Executing <Task(SSHOperator): spark_submit> on 2024-05-10 20:03:25.950289+00:00
[2024-05-10T20:03:27.977+0000] {standard_task_runner.py:52} INFO - Started process 226 to run task
[2024-05-10T20:03:27.981+0000] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 'pyspark_***_demo_dag', 'spark_submit', 'manual__2024-05-10T20:03:25.950289+00:00', '--job-id', '14', '--raw', '--subdir', 'DAGS_FOLDER/insert_data.py', '--cfg-path', '/tmp/tmps_kv8hp7', '--error-file', '/tmp/tmpjx28_wu2']
[2024-05-10T20:03:27.982+0000] {standard_task_runner.py:80} INFO - Job 14: Subtask spark_submit
[2024-05-10T20:03:28.053+0000] {task_command.py:371} INFO - Running <TaskInstance: pyspark_airflow_demo_dag.spark_submit manual__2024-05-10T20:03:25.950289+00:00 [running]> on host b9ef0bd34173
[2024-05-10T20:03:28.134+0000] {taskinstance.py:1583} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=pyspark_***_demo_dag
AIRFLOW_CTX_TASK_ID=spark_submit
AIRFLOW_CTX_EXECUTION_DATE=2024-05-10T20:03:25.950289+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=manual__2024-05-10T20:03:25.950289+00:00
[2024-05-10T20:03:28.135+0000] {ssh.py:136} INFO - Creating ssh_client
[2024-05-10T20:03:28.402+0000] {ssh.py:114} INFO - ssh_hook is not provided or invalid. Trying ssh_conn_id to create SSHHook.
[2024-05-10T20:03:28.413+0000] {base.py:68} INFO - Using connection ID 'ssh_spark' for task execution.
[2024-05-10T20:03:28.414+0000] {ssh.py:287} WARNING - No Host Key Verification. This won't protect against Man-In-The-Middle attacks
[2024-05-10T20:03:28.426+0000] {transport.py:1874} INFO - Connected (version 2.0, client OpenSSH_8.4p1)
[2024-05-10T20:03:28.490+0000] {transport.py:1874} INFO - Authentication (publickey) failed.
[2024-05-10T20:03:28.538+0000] {transport.py:1874} INFO - Authentication (password) successful!
[2024-05-10T20:03:28.539+0000] {ssh.py:464} INFO - Running command: . /home/spark_user/docker_env.txt && spark-submit             --master spark://spark:7077             --deploy-mode client             --executor-memory 4g             --executor-cores 2             --driver-cores 1            --driver-memory 1g             /opt/bitnami/spark/dev/scripts/pyspark_trial.py
            
[2024-05-10T20:03:30.439+0000] {ssh.py:496} INFO - :: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2024-05-10T20:03:30.513+0000] {ssh.py:500} WARNING - Ivy Default Cache set to: /home/spark_user/.ivy2/cache
The jars for the packages stored in: /home/spark_user/.ivy2/jars
[2024-05-10T20:03:30.520+0000] {ssh.py:500} WARNING - io.delta#delta-core_2.12 added as a dependency
org.apache.hadoop#hadoop-aws added as a dependency
[2024-05-10T20:03:30.521+0000] {ssh.py:500} WARNING - :: resolving dependencies :: org.apache.spark#spark-submit-parent-a81996e8-1e67-4c73-b425-fa95b627e654;1.0
	confs: [default]
[2024-05-10T20:03:32.562+0000] {ssh.py:500} WARNING - 	found io.delta#delta-core_2.12;2.4.0 in central
[2024-05-10T20:03:32.563+0000] {ssh.py:500} WARNING - 
[2024-05-10T20:03:33.000+0000] {ssh.py:500} WARNING - 	found io.delta#delta-storage;2.4.0 in central
[2024-05-10T20:03:39.647+0000] {ssh.py:500} WARNING - 	found org.antlr#antlr4-runtime;4.9.3 in central
[2024-05-10T20:03:43.819+0000] {ssh.py:500} WARNING - 	found org.apache.hadoop#hadoop-aws;3.3.6 in central
[2024-05-10T20:03:45.937+0000] {ssh.py:500} WARNING - 	found com.amazonaws#aws-java-sdk-bundle;1.12.367 in central
[2024-05-10T20:03:49.541+0000] {ssh.py:500} WARNING - 	found org.wildfly.openssl#wildfly-openssl;1.1.3.Final in central
[2024-05-10T20:03:49.750+0000] {ssh.py:500} WARNING - downloading https://repo1.maven.org/maven2/io/delta/delta-core_2.12/2.4.0/delta-core_2.12-2.4.0.jar ...
[2024-05-10T20:03:51.313+0000] {ssh.py:500} WARNING - 	[SUCCESSFUL ] io.delta#delta-core_2.12;2.4.0!delta-core_2.12.jar (1762ms)
[2024-05-10T20:03:51.507+0000] {ssh.py:500} WARNING - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.6/hadoop-aws-3.3.6.jar ...
[2024-05-10T20:03:51.807+0000] {ssh.py:500} WARNING - 	[SUCCESSFUL ] org.apache.hadoop#hadoop-aws;3.3.6!hadoop-aws.jar (493ms)
[2024-05-10T20:03:52.003+0000] {ssh.py:500} WARNING - downloading https://repo1.maven.org/maven2/io/delta/delta-storage/2.4.0/delta-storage-2.4.0.jar ...
[2024-05-10T20:03:52.204+0000] {ssh.py:500} WARNING - 	[SUCCESSFUL ] io.delta#delta-storage;2.4.0!delta-storage.jar (395ms)
[2024-05-10T20:03:52.402+0000] {ssh.py:500} WARNING - downloading https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.9.3/antlr4-runtime-4.9.3.jar ...
[2024-05-10T20:03:52.632+0000] {ssh.py:500} WARNING - 	[SUCCESSFUL ] org.antlr#antlr4-runtime;4.9.3!antlr4-runtime.jar (427ms)
[2024-05-10T20:03:52.830+0000] {ssh.py:500} WARNING - downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.367/aws-java-sdk-bundle-1.12.367.jar ...
[2024-05-10T20:05:12.553+0000] {ssh.py:500} WARNING - 	[SUCCESSFUL ] com.amazonaws#aws-java-sdk-bundle;1.12.367!aws-java-sdk-bundle.jar (79921ms)
[2024-05-10T20:05:12.748+0000] {ssh.py:500} WARNING - downloading https://repo1.maven.org/maven2/org/wildfly/openssl/wildfly-openssl/1.1.3.Final/wildfly-openssl-1.1.3.Final.jar ...
[2024-05-10T20:05:13.032+0000] {ssh.py:500} WARNING - 	[SUCCESSFUL ] org.wildfly.openssl#wildfly-openssl;1.1.3.Final!wildfly-openssl.jar (478ms)
[2024-05-10T20:05:13.033+0000] {ssh.py:500} WARNING - :: resolution report :: resolve 19029ms :: artifacts dl 83483ms
	:: modules in use:
	com.amazonaws#aws-java-sdk-bundle;1.12.367 from central in [default]
	io.delta#delta-core_2.12;2.4.0 from central in [default]
[2024-05-10T20:05:13.034+0000] {ssh.py:500} WARNING - 	io.delta#delta-storage;2.4.0 from central in [default]
	org.antlr#antlr4-runtime;4.9.3 from central in [default]
	org.apache.hadoop#hadoop-aws;3.3.6 from central in [default]
	org.wildfly.openssl#wildfly-openssl;1.1.3.Final from central in [default]
[2024-05-10T20:05:13.037+0000] {ssh.py:500} WARNING - 	---------------------------------------------------------------------
[2024-05-10T20:05:13.038+0000] {ssh.py:500} WARNING - 	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   6   |   6   |   6   |   0   ||   6   |   6   |
	---------------------------------------------------------------------
[2024-05-10T20:05:13.039+0000] {ssh.py:500} WARNING - :: retrieving :: org.apache.spark#spark-submit-parent-a81996e8-1e67-4c73-b425-fa95b627e654
	confs: [default]
[2024-05-10T20:05:13.500+0000] {ssh.py:500} WARNING - 	6 artifacts copied, 0 already retrieved (309034kB/461ms)
[2024-05-10T20:05:13.744+0000] {ssh.py:496} INFO - 24/05/10 20:05:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-05-10T20:05:14.658+0000] {ssh.py:496} INFO - 24/05/10 20:05:14 INFO SparkContext: Running Spark version 3.3.0
[2024-05-10T20:05:14.683+0000] {ssh.py:496} INFO - 24/05/10 20:05:14 INFO ResourceUtils: ==============================================================
[2024-05-10T20:05:14.684+0000] {ssh.py:496} INFO - 24/05/10 20:05:14 INFO ResourceUtils: No custom resources configured for spark.driver.
24/05/10 20:05:14 INFO ResourceUtils: ==============================================================
[2024-05-10T20:05:14.685+0000] {ssh.py:496} INFO - 24/05/10 20:05:14 INFO SparkContext: Submitted application: SparkAirFlowDemo
[2024-05-10T20:05:14.709+0000] {ssh.py:496} INFO - 24/05/10 20:05:14 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 2, script: , vendor: , memory -> name: memory, amount: 4096, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-05-10T20:05:14.725+0000] {ssh.py:496} INFO - 24/05/10 20:05:14 INFO ResourceProfile: Limiting resource is cpus at 2 tasks per executor
[2024-05-10T20:05:14.727+0000] {ssh.py:496} INFO - 24/05/10 20:05:14 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-05-10T20:05:14.793+0000] {ssh.py:496} INFO - 24/05/10 20:05:14 INFO SecurityManager: Changing view acls to: spark_user
[2024-05-10T20:05:14.794+0000] {ssh.py:496} INFO - 24/05/10 20:05:14 INFO SecurityManager: Changing modify acls to: spark_user
[2024-05-10T20:05:14.795+0000] {ssh.py:496} INFO - 24/05/10 20:05:14 INFO SecurityManager: Changing view acls groups to: 
24/05/10 20:05:14 INFO SecurityManager: Changing modify acls groups to: 
[2024-05-10T20:05:14.795+0000] {ssh.py:496} INFO - 24/05/10 20:05:14 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark_user); groups with view permissions: Set(); users  with modify permissions: Set(spark_user); groups with modify permissions: Set()
[2024-05-10T20:05:15.049+0000] {ssh.py:496} INFO - 24/05/10 20:05:15 INFO Utils: Successfully started service 'sparkDriver' on port 45261.
[2024-05-10T20:05:15.094+0000] {ssh.py:496} INFO - 24/05/10 20:05:15 INFO SparkEnv: Registering MapOutputTracker
[2024-05-10T20:05:15.142+0000] {ssh.py:496} INFO - 24/05/10 20:05:15 INFO SparkEnv: Registering BlockManagerMaster
[2024-05-10T20:05:15.166+0000] {ssh.py:496} INFO - 24/05/10 20:05:15 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-05-10T20:05:15.167+0000] {ssh.py:496} INFO - 24/05/10 20:05:15 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-05-10T20:05:15.172+0000] {ssh.py:496} INFO - 24/05/10 20:05:15 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-05-10T20:05:15.205+0000] {ssh.py:496} INFO - 24/05/10 20:05:15 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-98c02097-3b63-4ef7-9191-d3e669f2c99e
[2024-05-10T20:05:15.229+0000] {ssh.py:496} INFO - 24/05/10 20:05:15 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2024-05-10T20:05:15.250+0000] {ssh.py:496} INFO - 24/05/10 20:05:15 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-05-10T20:05:15.534+0000] {ssh.py:496} INFO - 24/05/10 20:05:15 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-05-10T20:05:15.581+0000] {ssh.py:496} INFO - 24/05/10 20:05:15 INFO SparkContext: Added JAR file:///home/spark_user/.ivy2/jars/io.delta_delta-core_2.12-2.4.0.jar at spark://acfda15750fe:45261/jars/io.delta_delta-core_2.12-2.4.0.jar with timestamp 1715371514649
24/05/10 20:05:15 INFO SparkContext: Added JAR file:///home/spark_user/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.6.jar at spark://acfda15750fe:45261/jars/org.apache.hadoop_hadoop-aws-3.3.6.jar with timestamp 1715371514649
24/05/10 20:05:15 INFO SparkContext: Added JAR file:///home/spark_user/.ivy2/jars/io.delta_delta-storage-2.4.0.jar at spark://acfda15750fe:45261/jars/io.delta_delta-storage-2.4.0.jar with timestamp 1715371514649
24/05/10 20:05:15 INFO SparkContext: Added JAR file:///home/spark_user/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar at spark://acfda15750fe:45261/jars/org.antlr_antlr4-runtime-4.9.3.jar with timestamp 1715371514649
[2024-05-10T20:05:15.582+0000] {ssh.py:496} INFO - 24/05/10 20:05:15 INFO SparkContext: Added JAR file:///home/spark_user/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.367.jar at spark://acfda15750fe:45261/jars/com.amazonaws_aws-java-sdk-bundle-1.12.367.jar with timestamp 1715371514649
24/05/10 20:05:15 INFO SparkContext: Added JAR file:///home/spark_user/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.1.3.Final.jar at spark://acfda15750fe:45261/jars/org.wildfly.openssl_wildfly-openssl-1.1.3.Final.jar with timestamp 1715371514649
[2024-05-10T20:05:15.585+0000] {ssh.py:496} INFO - 24/05/10 20:05:15 INFO SparkContext: Added file file:///home/spark_user/.ivy2/jars/io.delta_delta-core_2.12-2.4.0.jar at spark://acfda15750fe:45261/files/io.delta_delta-core_2.12-2.4.0.jar with timestamp 1715371514649
[2024-05-10T20:05:15.587+0000] {ssh.py:496} INFO - 24/05/10 20:05:15 INFO Utils: Copying /home/spark_user/.ivy2/jars/io.delta_delta-core_2.12-2.4.0.jar to /tmp/spark-5d2fcb3d-ebca-41c1-839e-854301e5710e/userFiles-a73e03ff-c288-408a-a6ac-bb7977de76f9/io.delta_delta-core_2.12-2.4.0.jar
[2024-05-10T20:05:15.608+0000] {ssh.py:496} INFO - 24/05/10 20:05:15 INFO SparkContext: Added file file:///home/spark_user/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.6.jar at spark://acfda15750fe:45261/files/org.apache.hadoop_hadoop-aws-3.3.6.jar with timestamp 1715371514649
[2024-05-10T20:05:15.608+0000] {ssh.py:496} INFO - 24/05/10 20:05:15 INFO Utils: Copying /home/spark_user/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.6.jar to /tmp/spark-5d2fcb3d-ebca-41c1-839e-854301e5710e/userFiles-a73e03ff-c288-408a-a6ac-bb7977de76f9/org.apache.hadoop_hadoop-aws-3.3.6.jar
[2024-05-10T20:05:15.615+0000] {ssh.py:496} INFO - 24/05/10 20:05:15 INFO SparkContext: Added file file:///home/spark_user/.ivy2/jars/io.delta_delta-storage-2.4.0.jar at spark://acfda15750fe:45261/files/io.delta_delta-storage-2.4.0.jar with timestamp 1715371514649
24/05/10 20:05:15 INFO Utils: Copying /home/spark_user/.ivy2/jars/io.delta_delta-storage-2.4.0.jar to /tmp/spark-5d2fcb3d-ebca-41c1-839e-854301e5710e/userFiles-a73e03ff-c288-408a-a6ac-bb7977de76f9/io.delta_delta-storage-2.4.0.jar
[2024-05-10T20:05:15.620+0000] {ssh.py:496} INFO - 24/05/10 20:05:15 INFO SparkContext: Added file file:///home/spark_user/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar at spark://acfda15750fe:45261/files/org.antlr_antlr4-runtime-4.9.3.jar with timestamp 1715371514649
[2024-05-10T20:05:15.621+0000] {ssh.py:496} INFO - 24/05/10 20:05:15 INFO Utils: Copying /home/spark_user/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar to /tmp/spark-5d2fcb3d-ebca-41c1-839e-854301e5710e/userFiles-a73e03ff-c288-408a-a6ac-bb7977de76f9/org.antlr_antlr4-runtime-4.9.3.jar
[2024-05-10T20:05:15.627+0000] {ssh.py:496} INFO - 24/05/10 20:05:15 INFO SparkContext: Added file file:///home/spark_user/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.367.jar at spark://acfda15750fe:45261/files/com.amazonaws_aws-java-sdk-bundle-1.12.367.jar with timestamp 1715371514649
[2024-05-10T20:05:15.628+0000] {ssh.py:496} INFO - 24/05/10 20:05:15 INFO Utils: Copying /home/spark_user/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.367.jar to /tmp/spark-5d2fcb3d-ebca-41c1-839e-854301e5710e/userFiles-a73e03ff-c288-408a-a6ac-bb7977de76f9/com.amazonaws_aws-java-sdk-bundle-1.12.367.jar
[2024-05-10T20:05:16.278+0000] {ssh.py:496} INFO - 24/05/10 20:05:16 INFO SparkContext: Added file file:///home/spark_user/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.1.3.Final.jar at spark://acfda15750fe:45261/files/org.wildfly.openssl_wildfly-openssl-1.1.3.Final.jar with timestamp 1715371514649
[2024-05-10T20:05:16.281+0000] {ssh.py:496} INFO - 24/05/10 20:05:16 INFO Utils: Copying /home/spark_user/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.1.3.Final.jar to /tmp/spark-5d2fcb3d-ebca-41c1-839e-854301e5710e/userFiles-a73e03ff-c288-408a-a6ac-bb7977de76f9/org.wildfly.openssl_wildfly-openssl-1.1.3.Final.jar
[2024-05-10T20:05:16.424+0000] {ssh.py:496} INFO - 24/05/10 20:05:16 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark:7077...
[2024-05-10T20:05:16.482+0000] {ssh.py:496} INFO - 24/05/10 20:05:16 INFO TransportClientFactory: Successfully created connection to spark/172.19.0.4:7077 after 37 ms (0 ms spent in bootstraps)
[2024-05-10T20:05:16.636+0000] {ssh.py:496} INFO - 24/05/10 20:05:16 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20240510200516-0000
[2024-05-10T20:05:16.646+0000] {ssh.py:496} INFO - 24/05/10 20:05:16 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36905.
[2024-05-10T20:05:16.649+0000] {ssh.py:496} INFO - 24/05/10 20:05:16 INFO NettyBlockTransferService: Server created on acfda15750fe:36905
24/05/10 20:05:16 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-05-10T20:05:16.662+0000] {ssh.py:496} INFO - 24/05/10 20:05:16 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, acfda15750fe, 36905, None)
[2024-05-10T20:05:16.666+0000] {ssh.py:496} INFO - 24/05/10 20:05:16 INFO BlockManagerMasterEndpoint: Registering block manager acfda15750fe:36905 with 366.3 MiB RAM, BlockManagerId(driver, acfda15750fe, 36905, None)
[2024-05-10T20:05:16.672+0000] {ssh.py:496} INFO - 24/05/10 20:05:16 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, acfda15750fe, 36905, None)
[2024-05-10T20:05:16.675+0000] {ssh.py:496} INFO - 24/05/10 20:05:16 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, acfda15750fe, 36905, None)
[2024-05-10T20:05:16.714+0000] {ssh.py:496} INFO - 24/05/10 20:05:16 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240510200516-0000/0 on worker-20240510200215-172.19.0.5-35417 (172.19.0.5:35417) with 2 core(s)
[2024-05-10T20:05:16.725+0000] {ssh.py:496} INFO - 24/05/10 20:05:16 INFO StandaloneSchedulerBackend: Granted executor ID app-20240510200516-0000/0 on hostPort 172.19.0.5:35417 with 2 core(s), 4.0 GiB RAM
[2024-05-10T20:05:16.727+0000] {ssh.py:496} INFO - 24/05/10 20:05:16 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240510200516-0000/1 on worker-20240510200215-172.19.0.6-45183 (172.19.0.6:45183) with 2 core(s)
[2024-05-10T20:05:16.733+0000] {ssh.py:496} INFO - 24/05/10 20:05:16 INFO StandaloneSchedulerBackend: Granted executor ID app-20240510200516-0000/1 on hostPort 172.19.0.6:45183 with 2 core(s), 4.0 GiB RAM
[2024-05-10T20:05:16.737+0000] {ssh.py:496} INFO - 24/05/10 20:05:16 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240510200516-0000/2 on worker-20240510200215-172.19.0.8-37217 (172.19.0.8:37217) with 2 core(s)
[2024-05-10T20:05:16.741+0000] {ssh.py:496} INFO - 24/05/10 20:05:16 INFO StandaloneSchedulerBackend: Granted executor ID app-20240510200516-0000/2 on hostPort 172.19.0.8:37217 with 2 core(s), 4.0 GiB RAM
[2024-05-10T20:05:17.033+0000] {ssh.py:496} INFO - 24/05/10 20:05:17 INFO SingleEventLogFileWriter: Logging events to file:/opt/bitnami/spark/logs/app-20240510200516-0000.inprogress
[2024-05-10T20:05:17.146+0000] {ssh.py:496} INFO - 24/05/10 20:05:17 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240510200516-0000/1 is now RUNNING
[2024-05-10T20:05:17.147+0000] {ssh.py:496} INFO - 24/05/10 20:05:17 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240510200516-0000/0 is now RUNNING
[2024-05-10T20:05:17.205+0000] {ssh.py:496} INFO - 24/05/10 20:05:17 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240510200516-0000/2 is now RUNNING
[2024-05-10T20:05:18.000+0000] {ssh.py:496} INFO - 24/05/10 20:05:17 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2024-05-10T20:05:27.271+0000] {ssh.py:496} INFO - root
 |-- firstname: string (nullable = true)
 |-- middlename: string (nullable = true)
 |-- lastname: string (nullable = true)
 |-- id: string (nullable = true)
 |-- gender: string (nullable = true)
 |-- salary: integer (nullable = true)
[2024-05-10T20:05:30.842+0000] {ssh.py:496} INFO - +---------+----------+--------+-----+------+------+
|firstname|middlename|lastname|id   |gender|salary|
+---------+----------+--------+-----+------+------+
|James    |          |Smith   |36636|M     |3000  |
|Michael  |Rose      |        |40288|M     |4000  |
|Robert   |          |Williams|42114|M     |4000  |
|Maria    |Anne      |Jones   |39192|F     |4000  |
|Jen      |Mary      |Brown   |     |F     |-1    |
+---------+----------+--------+-----+------+------+
[2024-05-10T20:05:31.612+0000] {ssh.py:496} INFO - Traceback (most recent call last):
  File "/opt/bitnami/spark/dev/scripts/pyspark_trial.py", line 57, in <module>
[2024-05-10T20:05:31.613+0000] {ssh.py:496} INFO -     main()
  File "/opt/bitnami/spark/dev/scripts/pyspark_trial.py", line 36, in main
[2024-05-10T20:05:31.614+0000] {ssh.py:496} INFO -     df.write.format("delta").option("delta.columnMapping.mode", "name").save(delta_path)
[2024-05-10T20:05:31.615+0000] {ssh.py:496} INFO -   File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 968, in save
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
[2024-05-10T20:05:31.615+0000] {ssh.py:496} INFO -   File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 190, in deco
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py", line 326, in get_return_value
[2024-05-10T20:05:31.615+0000] {ssh.py:496} INFO - py4j.protocol.Py4JJavaError
[2024-05-10T20:05:31.619+0000] {ssh.py:496} INFO - : An error occurred while calling o57.save.
: java.lang.NoClassDefFoundError: org/apache/spark/sql/catalyst/expressions/Empty2Null
	at org.apache.spark.sql.delta.DeltaLog.startTransaction(DeltaLog.scala:214)
	at org.apache.spark.sql.delta.DeltaLog.startTransaction(DeltaLog.scala:211)
	at org.apache.spark.sql.delta.DeltaLog.withNewTransaction(DeltaLog.scala:227)
	at org.apache.spark.sql.delta.commands.WriteIntoDelta.run(WriteIntoDelta.scala:93)
	at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:180)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:357)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.catalyst.expressions.Empty2Null
	at java.net.URLClassLoader.findClass(URLClassLoader.java:387)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	... 46 more
[2024-05-10T20:05:31.878+0000] {taskinstance.py:1902} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/ssh/operators/ssh.py", line 171, in execute
    result = self.run_ssh_client_command(ssh_client, self.command)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/ssh/operators/ssh.py", line 159, in run_ssh_client_command
    self.raise_for_status(exit_status, agg_stderr)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/ssh/operators/ssh.py", line 152, in raise_for_status
    raise AirflowException(f"SSH operator error: exit status = {exit_status}")
airflow.exceptions.AirflowException: SSH operator error: exit status = 1
[2024-05-10T20:05:31.884+0000] {taskinstance.py:1412} INFO - Marking task as FAILED. dag_id=pyspark_***_demo_dag, task_id=spark_submit, execution_date=20240510T200325, start_date=20240510T200327, end_date=20240510T200531
[2024-05-10T20:05:31.900+0000] {standard_task_runner.py:97} ERROR - Failed to execute job 14 for task spark_submit (SSH operator error: exit status = 1; 226)
[2024-05-10T20:05:31.928+0000] {local_task_job.py:156} INFO - Task exited with return code 1
[2024-05-10T20:05:31.972+0000] {local_task_job.py:279} INFO - 0 downstream tasks scheduled from follow-on schedule check
