[2024-05-09T23:03:15.341+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: pyspark_dag.pyspark_minio manual__2024-05-09T23:00:58.299253+00:00 [queued]>
[2024-05-09T23:03:15.360+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: pyspark_dag.pyspark_minio manual__2024-05-09T23:00:58.299253+00:00 [queued]>
[2024-05-09T23:03:15.361+0000] {taskinstance.py:1361} INFO - Starting attempt 2 of 2
[2024-05-09T23:03:15.394+0000] {taskinstance.py:1382} INFO - Executing <Task(DockerOperator): pyspark_minio> on 2024-05-09 23:00:58.299253+00:00
[2024-05-09T23:03:15.405+0000] {standard_task_runner.py:57} INFO - Started process 7500 to run task
[2024-05-09T23:03:15.411+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'pyspark_dag', 'pyspark_minio', 'manual__2024-05-09T23:00:58.299253+00:00', '--job-id', '64', '--raw', '--subdir', 'DAGS_FOLDER/dag_spark.py', '--cfg-path', '/tmp/tmp30c7q7wp']
[2024-05-09T23:03:15.416+0000] {standard_task_runner.py:85} INFO - Job 64: Subtask pyspark_minio
[2024-05-09T23:03:15.477+0000] {task_command.py:416} INFO - Running <TaskInstance: pyspark_dag.pyspark_minio manual__2024-05-09T23:00:58.299253+00:00 [running]> on host a7c0cd4e1305
[2024-05-09T23:03:15.555+0000] {taskinstance.py:1662} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='pyspark_dag' AIRFLOW_CTX_TASK_ID='pyspark_minio' AIRFLOW_CTX_EXECUTION_DATE='2024-05-09T23:00:58.299253+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-05-09T23:00:58.299253+00:00'
[2024-05-09T23:03:15.580+0000] {docker.py:343} INFO - Starting docker container from image wba-demo/spark:latest
[2024-05-09T23:03:15.584+0000] {docker.py:351} WARNING - Using remote engine or docker-in-docker and mounting temporary volume from host is not supported. Falling back to `mount_tmp_dir=False` mode. You can set `mount_tmp_dir` parameter to False to disable mounting and remove the warning
[2024-05-09T23:03:16.078+0000] {docker.py:413} INFO - [38;5;6mspark [38;5;5m23:03:16.07 [0m[38;5;2mINFO [0m ==>
[2024-05-09T23:03:16.079+0000] {docker.py:413} INFO - [38;5;6mspark [38;5;5m23:03:16.07 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
[2024-05-09T23:03:16.081+0000] {docker.py:413} INFO - [38;5;6mspark [38;5;5m23:03:16.08 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
[2024-05-09T23:03:16.083+0000] {docker.py:413} INFO - [38;5;6mspark [38;5;5m23:03:16.08 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
[2024-05-09T23:03:16.085+0000] {docker.py:413} INFO - [38;5;6mspark [38;5;5m23:03:16.08 [0m[38;5;2mINFO [0m ==>
[2024-05-09T23:03:16.091+0000] {docker.py:413} INFO - 
[2024-05-09T23:03:20.219+0000] {docker.py:413} INFO - :: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2024-05-09T23:03:20.319+0000] {docker.py:413} INFO - Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache
The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars
[2024-05-09T23:03:20.326+0000] {docker.py:413} INFO - io.delta#delta-core_2.12 added as a dependency
org.apache.hadoop#hadoop-aws added as a dependency
[2024-05-09T23:03:20.328+0000] {docker.py:413} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-063fb805-00fe-4651-85cf-e4f59f4ce951;1.0
[2024-05-09T23:03:20.329+0000] {docker.py:413} INFO - confs: [default]
[2024-05-09T23:03:22.520+0000] {docker.py:413} INFO - found io.delta#delta-core_2.12;2.4.0 in central
[2024-05-09T23:03:23.108+0000] {docker.py:413} INFO - found io.delta#delta-storage;2.4.0 in central
[2024-05-09T23:03:26.908+0000] {docker.py:413} INFO - found org.antlr#antlr4-runtime;4.9.3 in central
[2024-05-09T23:03:31.481+0000] {docker.py:413} INFO - found org.apache.hadoop#hadoop-aws;3.3.6 in central
[2024-05-09T23:03:33.969+0000] {docker.py:413} INFO - found com.amazonaws#aws-java-sdk-bundle;1.12.367 in central
[2024-05-09T23:03:38.203+0000] {docker.py:413} INFO - found org.wildfly.openssl#wildfly-openssl;1.1.3.Final in central
[2024-05-09T23:03:38.483+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/io/delta/delta-core_2.12/2.4.0/delta-core_2.12-2.4.0.jar ...
[2024-05-09T23:03:42.780+0000] {docker.py:413} INFO - [SUCCESSFUL ] io.delta#delta-core_2.12;2.4.0!delta-core_2.12.jar (4529ms)
[2024-05-09T23:03:43.014+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.6/hadoop-aws-3.3.6.jar ...
[2024-05-09T23:03:43.711+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.apache.hadoop#hadoop-aws;3.3.6!hadoop-aws.jar (925ms)
[2024-05-09T23:03:43.950+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/io/delta/delta-storage/2.4.0/delta-storage-2.4.0.jar ...
[2024-05-09T23:03:44.189+0000] {docker.py:413} INFO - [SUCCESSFUL ] io.delta#delta-storage;2.4.0!delta-storage.jar (466ms)
[2024-05-09T23:03:44.417+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.9.3/antlr4-runtime-4.9.3.jar ...
[2024-05-09T23:03:44.799+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.antlr#antlr4-runtime;4.9.3!antlr4-runtime.jar (610ms)
[2024-05-09T23:03:45.026+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.367/aws-java-sdk-bundle-1.12.367.jar ...
[2024-05-09T23:05:12.312+0000] {docker.py:413} INFO - [SUCCESSFUL ] com.amazonaws#aws-java-sdk-bundle;1.12.367!aws-java-sdk-bundle.jar (87510ms)
[2024-05-09T23:05:12.544+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/wildfly/openssl/wildfly-openssl/1.1.3.Final/wildfly-openssl-1.1.3.Final.jar ...
[2024-05-09T23:05:12.880+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.wildfly.openssl#wildfly-openssl;1.1.3.Final!wildfly-openssl.jar (564ms)
[2024-05-09T23:05:12.883+0000] {docker.py:413} INFO - :: resolution report :: resolve 17916ms :: artifacts dl 94638ms
[2024-05-09T23:05:12.884+0000] {docker.py:413} INFO - :: modules in use:
[2024-05-09T23:05:12.886+0000] {docker.py:413} INFO - com.amazonaws#aws-java-sdk-bundle;1.12.367 from central in [default]
	io.delta#delta-core_2.12;2.4.0 from central in [default]
	io.delta#delta-storage;2.4.0 from central in [default]
	org.antlr#antlr4-runtime;4.9.3 from central in [default]
	org.apache.hadoop#hadoop-aws;3.3.6 from central in [default]
	org.wildfly.openssl#wildfly-openssl;1.1.3.Final from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   6   |   6   |   6   |   0   ||   6   |   6   |
	---------------------------------------------------------------------
[2024-05-09T23:05:12.899+0000] {docker.py:413} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-063fb805-00fe-4651-85cf-e4f59f4ce951
	confs: [default]
[2024-05-09T23:05:13.160+0000] {docker.py:413} INFO - 6 artifacts copied, 0 already retrieved (309034kB/262ms)
[2024-05-09T23:05:13.488+0000] {docker.py:413} INFO - 24/05/09 23:05:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-05-09T23:05:14.980+0000] {docker.py:413} INFO - 24/05/09 23:05:14 INFO SparkContext: Running Spark version 3.4.1
[2024-05-09T23:05:15.098+0000] {docker.py:413} INFO - 24/05/09 23:05:15 INFO ResourceUtils: ==============================================================
[2024-05-09T23:05:15.103+0000] {docker.py:413} INFO - 24/05/09 23:05:15 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-05-09T23:05:15.106+0000] {docker.py:413} INFO - 24/05/09 23:05:15 INFO ResourceUtils: ==============================================================
[2024-05-09T23:05:15.110+0000] {docker.py:413} INFO - 24/05/09 23:05:15 INFO SparkContext: Submitted application: CSV File to Delta Lake Table
[2024-05-09T23:05:15.166+0000] {docker.py:413} INFO - 24/05/09 23:05:15 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-05-09T23:05:15.185+0000] {docker.py:413} INFO - 24/05/09 23:05:15 INFO ResourceProfile: Limiting resource is cpu
[2024-05-09T23:05:15.187+0000] {docker.py:413} INFO - 24/05/09 23:05:15 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-05-09T23:05:15.380+0000] {docker.py:413} INFO - 24/05/09 23:05:15 INFO SecurityManager: Changing view acls to: spark
[2024-05-09T23:05:15.381+0000] {docker.py:413} INFO - 24/05/09 23:05:15 INFO SecurityManager: Changing modify acls to: spark
[2024-05-09T23:05:15.382+0000] {docker.py:413} INFO - 24/05/09 23:05:15 INFO SecurityManager: Changing view acls groups to:
[2024-05-09T23:05:15.384+0000] {docker.py:413} INFO - 24/05/09 23:05:15 INFO SecurityManager: Changing modify acls groups to:
[2024-05-09T23:05:15.385+0000] {docker.py:413} INFO - 24/05/09 23:05:15 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
[2024-05-09T23:05:15.747+0000] {docker.py:413} INFO - 24/05/09 23:05:15 INFO Utils: Successfully started service 'sparkDriver' on port 38743.
[2024-05-09T23:05:15.807+0000] {docker.py:413} INFO - 24/05/09 23:05:15 INFO SparkEnv: Registering MapOutputTracker
[2024-05-09T23:05:15.863+0000] {docker.py:413} INFO - 24/05/09 23:05:15 INFO SparkEnv: Registering BlockManagerMaster
[2024-05-09T23:05:15.886+0000] {docker.py:413} INFO - 24/05/09 23:05:15 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-05-09T23:05:15.887+0000] {docker.py:413} INFO - 24/05/09 23:05:15 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-05-09T23:05:15.893+0000] {docker.py:413} INFO - 24/05/09 23:05:15 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-05-09T23:05:15.924+0000] {docker.py:413} INFO - 24/05/09 23:05:15 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d45586e3-2429-4b5f-ab3f-178ae841c5da
[2024-05-09T23:05:15.948+0000] {docker.py:413} INFO - 24/05/09 23:05:15 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2024-05-09T23:05:15.961+0000] {docker.py:413} INFO - 24/05/09 23:05:15 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-05-09T23:05:16.137+0000] {docker.py:413} INFO - 24/05/09 23:05:16 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2024-05-09T23:05:16.333+0000] {docker.py:413} INFO - 24/05/09 23:05:16 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-05-09T23:05:16.433+0000] {docker.py:413} INFO - 24/05/09 23:05:16 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/io.delta_delta-core_2.12-2.4.0.jar at spark://localhost:38743/jars/io.delta_delta-core_2.12-2.4.0.jar with timestamp 1715295914943
[2024-05-09T23:05:16.434+0000] {docker.py:413} INFO - 24/05/09 23:05:16 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.6.jar at spark://localhost:38743/jars/org.apache.hadoop_hadoop-aws-3.3.6.jar with timestamp 1715295914943
[2024-05-09T23:05:16.434+0000] {docker.py:413} INFO - 24/05/09 23:05:16 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/io.delta_delta-storage-2.4.0.jar at spark://localhost:38743/jars/io.delta_delta-storage-2.4.0.jar with timestamp 1715295914943
[2024-05-09T23:05:16.435+0000] {docker.py:413} INFO - 24/05/09 23:05:16 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar at spark://localhost:38743/jars/org.antlr_antlr4-runtime-4.9.3.jar with timestamp 1715295914943
[2024-05-09T23:05:16.435+0000] {docker.py:413} INFO - 24/05/09 23:05:16 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.367.jar at spark://localhost:38743/jars/com.amazonaws_aws-java-sdk-bundle-1.12.367.jar with timestamp 1715295914943
[2024-05-09T23:05:16.436+0000] {docker.py:413} INFO - 24/05/09 23:05:16 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.1.3.Final.jar at spark://localhost:38743/jars/org.wildfly.openssl_wildfly-openssl-1.1.3.Final.jar with timestamp 1715295914943
[2024-05-09T23:05:16.438+0000] {docker.py:413} INFO - 24/05/09 23:05:16 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/io.delta_delta-core_2.12-2.4.0.jar at file:///opt/bitnami/spark/.ivy2/jars/io.delta_delta-core_2.12-2.4.0.jar with timestamp 1715295914943
[2024-05-09T23:05:16.440+0000] {docker.py:413} INFO - 24/05/09 23:05:16 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/io.delta_delta-core_2.12-2.4.0.jar to /tmp/spark-55047e66-4219-42e8-8513-e1095c4ac036/userFiles-0d427e0a-e187-4b25-a94a-1a916f1e4f25/io.delta_delta-core_2.12-2.4.0.jar
[2024-05-09T23:05:16.456+0000] {docker.py:413} INFO - 24/05/09 23:05:16 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.6.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.6.jar with timestamp 1715295914943
[2024-05-09T23:05:16.456+0000] {docker.py:413} INFO - 24/05/09 23:05:16 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.6.jar to /tmp/spark-55047e66-4219-42e8-8513-e1095c4ac036/userFiles-0d427e0a-e187-4b25-a94a-1a916f1e4f25/org.apache.hadoop_hadoop-aws-3.3.6.jar
[2024-05-09T23:05:16.461+0000] {docker.py:413} INFO - 24/05/09 23:05:16 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/io.delta_delta-storage-2.4.0.jar at file:///opt/bitnami/spark/.ivy2/jars/io.delta_delta-storage-2.4.0.jar with timestamp 1715295914943
[2024-05-09T23:05:16.462+0000] {docker.py:413} INFO - 24/05/09 23:05:16 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/io.delta_delta-storage-2.4.0.jar to /tmp/spark-55047e66-4219-42e8-8513-e1095c4ac036/userFiles-0d427e0a-e187-4b25-a94a-1a916f1e4f25/io.delta_delta-storage-2.4.0.jar
[2024-05-09T23:05:16.466+0000] {docker.py:413} INFO - 24/05/09 23:05:16 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar at file:///opt/bitnami/spark/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar with timestamp 1715295914943
[2024-05-09T23:05:16.467+0000] {docker.py:413} INFO - 24/05/09 23:05:16 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar to /tmp/spark-55047e66-4219-42e8-8513-e1095c4ac036/userFiles-0d427e0a-e187-4b25-a94a-1a916f1e4f25/org.antlr_antlr4-runtime-4.9.3.jar
[2024-05-09T23:05:16.472+0000] {docker.py:413} INFO - 24/05/09 23:05:16 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.367.jar at file:///opt/bitnami/spark/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.367.jar with timestamp 1715295914943
[2024-05-09T23:05:16.473+0000] {docker.py:413} INFO - 24/05/09 23:05:16 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.367.jar to /tmp/spark-55047e66-4219-42e8-8513-e1095c4ac036/userFiles-0d427e0a-e187-4b25-a94a-1a916f1e4f25/com.amazonaws_aws-java-sdk-bundle-1.12.367.jar
[2024-05-09T23:05:16.728+0000] {docker.py:413} INFO - 24/05/09 23:05:16 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.1.3.Final.jar at file:///opt/bitnami/spark/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.1.3.Final.jar with timestamp 1715295914943
[2024-05-09T23:05:16.728+0000] {docker.py:413} INFO - 24/05/09 23:05:16 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.1.3.Final.jar to /tmp/spark-55047e66-4219-42e8-8513-e1095c4ac036/userFiles-0d427e0a-e187-4b25-a94a-1a916f1e4f25/org.wildfly.openssl_wildfly-openssl-1.1.3.Final.jar
[2024-05-09T23:05:16.812+0000] {docker.py:413} INFO - 24/05/09 23:05:16 INFO Executor: Starting executor ID driver on host localhost
[2024-05-09T23:05:16.818+0000] {docker.py:413} INFO - 24/05/09 23:05:16 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2024-05-09T23:05:16.829+0000] {docker.py:413} INFO - 24/05/09 23:05:16 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.6.jar with timestamp 1715295914943
[2024-05-09T23:05:16.852+0000] {docker.py:413} INFO - 24/05/09 23:05:16 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.6.jar has been previously copied to /tmp/spark-55047e66-4219-42e8-8513-e1095c4ac036/userFiles-0d427e0a-e187-4b25-a94a-1a916f1e4f25/org.apache.hadoop_hadoop-aws-3.3.6.jar
[2024-05-09T23:05:16.856+0000] {docker.py:413} INFO - 24/05/09 23:05:16 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/io.delta_delta-core_2.12-2.4.0.jar with timestamp 1715295914943
[2024-05-09T23:05:16.861+0000] {docker.py:413} INFO - 24/05/09 23:05:16 INFO Utils: /opt/bitnami/spark/.ivy2/jars/io.delta_delta-core_2.12-2.4.0.jar has been previously copied to /tmp/spark-55047e66-4219-42e8-8513-e1095c4ac036/userFiles-0d427e0a-e187-4b25-a94a-1a916f1e4f25/io.delta_delta-core_2.12-2.4.0.jar
[2024-05-09T23:05:16.875+0000] {docker.py:413} INFO - 24/05/09 23:05:16 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/io.delta_delta-storage-2.4.0.jar with timestamp 1715295914943
[2024-05-09T23:05:16.876+0000] {docker.py:413} INFO - 24/05/09 23:05:16 INFO Utils: /opt/bitnami/spark/.ivy2/jars/io.delta_delta-storage-2.4.0.jar has been previously copied to /tmp/spark-55047e66-4219-42e8-8513-e1095c4ac036/userFiles-0d427e0a-e187-4b25-a94a-1a916f1e4f25/io.delta_delta-storage-2.4.0.jar
[2024-05-09T23:05:16.880+0000] {docker.py:413} INFO - 24/05/09 23:05:16 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar with timestamp 1715295914943
[2024-05-09T23:05:16.881+0000] {docker.py:413} INFO - 24/05/09 23:05:16 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar has been previously copied to /tmp/spark-55047e66-4219-42e8-8513-e1095c4ac036/userFiles-0d427e0a-e187-4b25-a94a-1a916f1e4f25/org.antlr_antlr4-runtime-4.9.3.jar
[2024-05-09T23:05:16.885+0000] {docker.py:413} INFO - 24/05/09 23:05:16 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.1.3.Final.jar with timestamp 1715295914943
[2024-05-09T23:05:16.886+0000] {docker.py:413} INFO - 24/05/09 23:05:16 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.1.3.Final.jar has been previously copied to /tmp/spark-55047e66-4219-42e8-8513-e1095c4ac036/userFiles-0d427e0a-e187-4b25-a94a-1a916f1e4f25/org.wildfly.openssl_wildfly-openssl-1.1.3.Final.jar
[2024-05-09T23:05:16.892+0000] {docker.py:413} INFO - 24/05/09 23:05:16 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.367.jar with timestamp 1715295914943
[2024-05-09T23:05:17.114+0000] {docker.py:413} INFO - 24/05/09 23:05:17 INFO Utils: /opt/bitnami/spark/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.367.jar has been previously copied to /tmp/spark-55047e66-4219-42e8-8513-e1095c4ac036/userFiles-0d427e0a-e187-4b25-a94a-1a916f1e4f25/com.amazonaws_aws-java-sdk-bundle-1.12.367.jar
[2024-05-09T23:05:17.120+0000] {docker.py:413} INFO - 24/05/09 23:05:17 INFO Executor: Fetching spark://localhost:38743/jars/io.delta_delta-core_2.12-2.4.0.jar with timestamp 1715295914943
[2024-05-09T23:05:17.174+0000] {docker.py:413} INFO - 24/05/09 23:05:17 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:38743 after 40 ms (0 ms spent in bootstraps)
[2024-05-09T23:05:17.184+0000] {docker.py:413} INFO - 24/05/09 23:05:17 INFO Utils: Fetching spark://localhost:38743/jars/io.delta_delta-core_2.12-2.4.0.jar to /tmp/spark-55047e66-4219-42e8-8513-e1095c4ac036/userFiles-0d427e0a-e187-4b25-a94a-1a916f1e4f25/fetchFileTemp4968321282263576751.tmp
[2024-05-09T23:05:17.236+0000] {docker.py:413} INFO - 24/05/09 23:05:17 INFO Utils: /tmp/spark-55047e66-4219-42e8-8513-e1095c4ac036/userFiles-0d427e0a-e187-4b25-a94a-1a916f1e4f25/fetchFileTemp4968321282263576751.tmp has been previously copied to /tmp/spark-55047e66-4219-42e8-8513-e1095c4ac036/userFiles-0d427e0a-e187-4b25-a94a-1a916f1e4f25/io.delta_delta-core_2.12-2.4.0.jar
[2024-05-09T23:05:17.241+0000] {docker.py:413} INFO - 24/05/09 23:05:17 INFO Executor: Adding file:/tmp/spark-55047e66-4219-42e8-8513-e1095c4ac036/userFiles-0d427e0a-e187-4b25-a94a-1a916f1e4f25/io.delta_delta-core_2.12-2.4.0.jar to class loader
[2024-05-09T23:05:17.241+0000] {docker.py:413} INFO - 24/05/09 23:05:17 INFO Executor: Fetching spark://localhost:38743/jars/io.delta_delta-storage-2.4.0.jar with timestamp 1715295914943
[2024-05-09T23:05:17.242+0000] {docker.py:413} INFO - 24/05/09 23:05:17 INFO Utils: Fetching spark://localhost:38743/jars/io.delta_delta-storage-2.4.0.jar to /tmp/spark-55047e66-4219-42e8-8513-e1095c4ac036/userFiles-0d427e0a-e187-4b25-a94a-1a916f1e4f25/fetchFileTemp18106136510162526484.tmp
[2024-05-09T23:05:17.244+0000] {docker.py:413} INFO - 24/05/09 23:05:17 INFO Utils: /tmp/spark-55047e66-4219-42e8-8513-e1095c4ac036/userFiles-0d427e0a-e187-4b25-a94a-1a916f1e4f25/fetchFileTemp18106136510162526484.tmp has been previously copied to /tmp/spark-55047e66-4219-42e8-8513-e1095c4ac036/userFiles-0d427e0a-e187-4b25-a94a-1a916f1e4f25/io.delta_delta-storage-2.4.0.jar
[2024-05-09T23:05:17.248+0000] {docker.py:413} INFO - 24/05/09 23:05:17 INFO Executor: Adding file:/tmp/spark-55047e66-4219-42e8-8513-e1095c4ac036/userFiles-0d427e0a-e187-4b25-a94a-1a916f1e4f25/io.delta_delta-storage-2.4.0.jar to class loader
[2024-05-09T23:05:17.248+0000] {docker.py:413} INFO - 24/05/09 23:05:17 INFO Executor: Fetching spark://localhost:38743/jars/org.antlr_antlr4-runtime-4.9.3.jar with timestamp 1715295914943
[2024-05-09T23:05:17.248+0000] {docker.py:413} INFO - 24/05/09 23:05:17 INFO Utils: Fetching spark://localhost:38743/jars/org.antlr_antlr4-runtime-4.9.3.jar to /tmp/spark-55047e66-4219-42e8-8513-e1095c4ac036/userFiles-0d427e0a-e187-4b25-a94a-1a916f1e4f25/fetchFileTemp13055802324359225314.tmp
[2024-05-09T23:05:17.252+0000] {docker.py:413} INFO - 24/05/09 23:05:17 INFO Utils: /tmp/spark-55047e66-4219-42e8-8513-e1095c4ac036/userFiles-0d427e0a-e187-4b25-a94a-1a916f1e4f25/fetchFileTemp13055802324359225314.tmp has been previously copied to /tmp/spark-55047e66-4219-42e8-8513-e1095c4ac036/userFiles-0d427e0a-e187-4b25-a94a-1a916f1e4f25/org.antlr_antlr4-runtime-4.9.3.jar
[2024-05-09T23:05:17.257+0000] {docker.py:413} INFO - 24/05/09 23:05:17 INFO Executor: Adding file:/tmp/spark-55047e66-4219-42e8-8513-e1095c4ac036/userFiles-0d427e0a-e187-4b25-a94a-1a916f1e4f25/org.antlr_antlr4-runtime-4.9.3.jar to class loader
[2024-05-09T23:05:17.258+0000] {docker.py:413} INFO - 24/05/09 23:05:17 INFO Executor: Fetching spark://localhost:38743/jars/org.apache.hadoop_hadoop-aws-3.3.6.jar with timestamp 1715295914943
[2024-05-09T23:05:17.259+0000] {docker.py:413} INFO - 24/05/09 23:05:17 INFO Utils: Fetching spark://localhost:38743/jars/org.apache.hadoop_hadoop-aws-3.3.6.jar to /tmp/spark-55047e66-4219-42e8-8513-e1095c4ac036/userFiles-0d427e0a-e187-4b25-a94a-1a916f1e4f25/fetchFileTemp45563438979915205.tmp
[2024-05-09T23:05:17.264+0000] {docker.py:413} INFO - 24/05/09 23:05:17 INFO Utils: /tmp/spark-55047e66-4219-42e8-8513-e1095c4ac036/userFiles-0d427e0a-e187-4b25-a94a-1a916f1e4f25/fetchFileTemp45563438979915205.tmp has been previously copied to /tmp/spark-55047e66-4219-42e8-8513-e1095c4ac036/userFiles-0d427e0a-e187-4b25-a94a-1a916f1e4f25/org.apache.hadoop_hadoop-aws-3.3.6.jar
[2024-05-09T23:05:17.269+0000] {docker.py:413} INFO - 24/05/09 23:05:17 INFO Executor: Adding file:/tmp/spark-55047e66-4219-42e8-8513-e1095c4ac036/userFiles-0d427e0a-e187-4b25-a94a-1a916f1e4f25/org.apache.hadoop_hadoop-aws-3.3.6.jar to class loader
[2024-05-09T23:05:17.269+0000] {docker.py:413} INFO - 24/05/09 23:05:17 INFO Executor: Fetching spark://localhost:38743/jars/com.amazonaws_aws-java-sdk-bundle-1.12.367.jar with timestamp 1715295914943
[2024-05-09T23:05:17.272+0000] {docker.py:413} INFO - 24/05/09 23:05:17 INFO Utils: Fetching spark://localhost:38743/jars/com.amazonaws_aws-java-sdk-bundle-1.12.367.jar to /tmp/spark-55047e66-4219-42e8-8513-e1095c4ac036/userFiles-0d427e0a-e187-4b25-a94a-1a916f1e4f25/fetchFileTemp13526417067127187660.tmp
[2024-05-09T23:05:19.134+0000] {docker.py:413} INFO - 24/05/09 23:05:19 INFO Utils: /tmp/spark-55047e66-4219-42e8-8513-e1095c4ac036/userFiles-0d427e0a-e187-4b25-a94a-1a916f1e4f25/fetchFileTemp13526417067127187660.tmp has been previously copied to /tmp/spark-55047e66-4219-42e8-8513-e1095c4ac036/userFiles-0d427e0a-e187-4b25-a94a-1a916f1e4f25/com.amazonaws_aws-java-sdk-bundle-1.12.367.jar
[2024-05-09T23:05:19.173+0000] {docker.py:413} INFO - 24/05/09 23:05:19 INFO Executor: Adding file:/tmp/spark-55047e66-4219-42e8-8513-e1095c4ac036/userFiles-0d427e0a-e187-4b25-a94a-1a916f1e4f25/com.amazonaws_aws-java-sdk-bundle-1.12.367.jar to class loader
[2024-05-09T23:05:19.173+0000] {docker.py:413} INFO - 24/05/09 23:05:19 INFO Executor: Fetching spark://localhost:38743/jars/org.wildfly.openssl_wildfly-openssl-1.1.3.Final.jar with timestamp 1715295914943
[2024-05-09T23:05:19.174+0000] {docker.py:413} INFO - 24/05/09 23:05:19 INFO Utils: Fetching spark://localhost:38743/jars/org.wildfly.openssl_wildfly-openssl-1.1.3.Final.jar to /tmp/spark-55047e66-4219-42e8-8513-e1095c4ac036/userFiles-0d427e0a-e187-4b25-a94a-1a916f1e4f25/fetchFileTemp348454544783974815.tmp
[2024-05-09T23:05:19.177+0000] {docker.py:413} INFO - 24/05/09 23:05:19 INFO Utils: /tmp/spark-55047e66-4219-42e8-8513-e1095c4ac036/userFiles-0d427e0a-e187-4b25-a94a-1a916f1e4f25/fetchFileTemp348454544783974815.tmp has been previously copied to /tmp/spark-55047e66-4219-42e8-8513-e1095c4ac036/userFiles-0d427e0a-e187-4b25-a94a-1a916f1e4f25/org.wildfly.openssl_wildfly-openssl-1.1.3.Final.jar
[2024-05-09T23:05:19.181+0000] {docker.py:413} INFO - 24/05/09 23:05:19 INFO Executor: Adding file:/tmp/spark-55047e66-4219-42e8-8513-e1095c4ac036/userFiles-0d427e0a-e187-4b25-a94a-1a916f1e4f25/org.wildfly.openssl_wildfly-openssl-1.1.3.Final.jar to class loader
[2024-05-09T23:05:19.192+0000] {docker.py:413} INFO - 24/05/09 23:05:19 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40895.
[2024-05-09T23:05:19.194+0000] {docker.py:413} INFO - 24/05/09 23:05:19 INFO NettyBlockTransferService: Server created on localhost:40895
[2024-05-09T23:05:19.196+0000] {docker.py:413} INFO - 24/05/09 23:05:19 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-05-09T23:05:19.205+0000] {docker.py:413} INFO - 24/05/09 23:05:19 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, localhost, 40895, None)
[2024-05-09T23:05:19.210+0000] {docker.py:413} INFO - 24/05/09 23:05:19 INFO BlockManagerMasterEndpoint: Registering block manager localhost:40895 with 434.4 MiB RAM, BlockManagerId(driver, localhost, 40895, None)
[2024-05-09T23:05:19.214+0000] {docker.py:413} INFO - 24/05/09 23:05:19 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, localhost, 40895, None)
[2024-05-09T23:05:19.216+0000] {docker.py:413} INFO - 24/05/09 23:05:19 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, localhost, 40895, None)
[2024-05-09T23:05:24.952+0000] {docker.py:413} INFO - root
 |-- firstname: string (nullable = true)
 |-- middlename: string (nullable = true)
 |-- lastname: string (nullable = true)
 |-- id: string (nullable = true)
 |-- gender: string (nullable = true)
 |-- salary: integer (nullable = true)
[2024-05-09T23:05:29.132+0000] {docker.py:413} INFO - +---------+----------+--------+-----+------+------+
|firstname|middlename|lastname|id   |gender|salary|
+---------+----------+--------+-----+------+------+
|James    |          |Smith   |36636|M     |3000  |
|Michael  |Rose      |        |40288|M     |4000  |
|Robert   |          |Williams|42114|M     |4000  |
|Maria    |Anne      |Jones   |39192|F     |4000  |
|Jen      |Mary      |Brown   |     |F     |-1    |
+---------+----------+--------+-----+------+------+
[2024-05-09T23:05:32.134+0000] {docker.py:413} INFO - +-----+---------------+----------+---------+------+--------------------+--------------------+-------------+--------------------+
|Index|        User Id|First Name|Last Name|   Sex|               Email|               Phone|Date of birth|           Job Title|
+-----+---------------+----------+---------+------+--------------------+--------------------+-------------+--------------------+
|    1|88F7B33d2bcf9f5|    Shelby|  Terrell|  Male|elijah57@example.net|001-084-906-7849x...|   1945-10-26|     Games developer|
|    2|f90cD3E76f1A9b9|   Phillip|  Summers|Female|bethany14@example...|   214.112.6044x4913|   1910-03-24|      Phytotherapist|
|    3|DbeAb8CcdfeFC2c|  Kristine|   Travis|  Male|bthompson@example...|        277.609.7938|   1992-07-02|           Homeopath|
|    4|A31Bee3c201ef58|   Yesenia| Martinez|  Male|kaitlinkaiser@exa...|        584.094.6111|   2017-08-03|   Market researcher|
|    5|1bA7A3dc874da3c|      Lori|     Todd|  Male|buchananmanuel@ex...|   689-207-3558x7233|   1938-12-01|  Veterinary surgeon|
|    6|bfDD7CDEF5D865B|      Erin|      Day|  Male| tconner@example.org|001-171-649-9856x...|   2015-10-28|Waste management ...|
|    7|bE9EEf34cB72AF7| Katherine|     Buck|Female|conniecowan@examp...|+1-773-151-6685x4...|   1989-01-22|Intelligence analyst|
|    8|2EFC6A4e77FaEaC|   Ricardo|   Hinton|  Male|wyattbishop@examp...|001-447-699-7998x...|   1924-03-26|      Hydrogeologist|
|    9|baDcC4DeefD8dEB|      Dave|  Farrell|  Male| nmccann@example.net|  603-428-2429x27392|   2018-10-06|              Lawyer|
|   10|8e4FB470FE19bF0|    Isaiah|    Downs|  Male|virginiaterrell@e...|+1-511-372-1544x8206|   1964-09-20|      Engineer, site|
|   11|BF0BbA03C29Bb3b|    Sheila|     Ross|Female|huangcathy@exampl...|        895.881.4746|   2008-03-20|Advertising accou...|
|   12|F738c69fB34E62E|     Stacy|   Newton|  Male|rayleroy@example.org|  710.673.3213x80335|   1980-10-20|       Warden/ranger|
|   13|C03fDADdAadAdCe|     Mandy|    Blake|  Male|jefferynoble@exam...|  (992)466-1305x4947|   2007-12-08|Scient
[2024-05-09T23:05:32.134+0000] {docker.py:413} INFO - ist, clinic...|
|   14|b759b74BD1dE80d|   Bridget|     Nash|Female|mercedes44@exampl...|       (216)627-8359|   2004-06-28|       Social worker|
|   15|1F0B7D65A00DAF9|   Crystal|   Farmer|  Male|pmiranda@example.org|     +1-024-377-5391|   1992-03-09|Agricultural cons...|
|   16|50Bb061cB30B461|    Thomas|   Knight|Female|braunpriscilla@ex...|     +1-360-880-0766|   2006-02-18|Sport and exercis...|
|   17|D6dbA5308fEC4BC|   Maurice|   Rangel|  Male|sheenabanks@examp...|       (246)187-4969|   2004-08-20|Secretary/adminis...|
|   18|311D775990f066d|     Frank|  Meadows|  Male| gbrewer@example.org|   429.965.3902x4447|   2008-09-16|Audiological scie...|
|   19|7F7E1BAcb0C9AFf|     Alvin|     Paul|  Male|gilbertdonaldson@...|  219.436.0887x07551|   1949-05-12|Teacher, adult ed...|
|   20|88473e15D5c3cD0|     Jared| Mitchell|Female| jcortez@example.com|     +1-958-849-6781|   1921-01-18|    Paediatric nurse|
+-----+---------------+----------+---------+------+--------------------+--------------------+-------------+--------------------+
only showing top 20 rows
[2024-05-09T23:05:32.679+0000] {docker.py:413} INFO - Traceback (most recent call last):
  File "/opt/spark-apps/csv_to_delta.py", line 56, in <module>
[2024-05-09T23:05:32.680+0000] {docker.py:413} INFO - main()
[2024-05-09T23:05:32.681+0000] {docker.py:413} INFO - File "/opt/spark-apps/csv_to_delta.py", line 45, in main
[2024-05-09T23:05:32.681+0000] {docker.py:413} INFO - df.write.format("delta").save(delta_path)
[2024-05-09T23:05:32.681+0000] {docker.py:413} INFO - File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 1398, in save
[2024-05-09T23:05:32.682+0000] {docker.py:413} INFO - File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
[2024-05-09T23:05:32.682+0000] {docker.py:413} INFO - File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
[2024-05-09T23:05:32.682+0000] {docker.py:413} INFO - pyspark.errors.exceptions.captured
[2024-05-09T23:05:32.683+0000] {docker.py:413} INFO - .
[2024-05-09T23:05:32.683+0000] {docker.py:413} INFO - AnalysisException
[2024-05-09T23:05:32.692+0000] {docker.py:413} INFO - : Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. 
Please enable Column Mapping on your Delta table with mapping mode 'name'.
You can use one of the following commands.

If your table is already on the required protocol version:
ALTER TABLE table_name SET TBLPROPERTIES ('delta.columnMapping.mode' = 'name')

If your table is not on the required protocol version and requires a protocol upgrade:
ALTER TABLE table_name SET TBLPROPERTIES (
   'delta.columnMapping.mode' = 'name',
   'delta.minReaderVersion' = '2',
   'delta.minWriterVersion' = '5')
[2024-05-09T23:05:33.688+0000] {taskinstance.py:1937} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/docker/api/client.py", line 268, in _raise_for_status
    response.raise_for_status()
  File "/home/airflow/.local/lib/python3.8/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 400 Client Error: Bad Request for url: http://docker-proxy:2375/v1.43/containers/create

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/docker/operators/docker.py", line 348, in _run_image
    return self._run_image_with_mounts([*self.mounts, tmp_mount], add_tmp_variable=True)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/docker/operators/docker.py", line 375, in _run_image_with_mounts
    self.container = self.cli.create_container(
  File "/home/airflow/.local/lib/python3.8/site-packages/docker/api/container.py", line 431, in create_container
    return self.create_container_from_config(config, name, platform)
  File "/home/airflow/.local/lib/python3.8/site-packages/docker/api/container.py", line 448, in create_container_from_config
    return self._result(res, True)
  File "/home/airflow/.local/lib/python3.8/site-packages/docker/api/client.py", line 274, in _result
    self._raise_for_status(response)
  File "/home/airflow/.local/lib/python3.8/site-packages/docker/api/client.py", line 270, in _raise_for_status
    raise create_api_error_from_http_exception(e) from e
  File "/home/airflow/.local/lib/python3.8/site-packages/docker/errors.py", line 39, in create_api_error_from_http_exception
    raise cls(e, response=response, explanation=explanation) from e
docker.errors.APIError: 400 Client Error for http://docker-proxy:2375/v1.43/containers/create: Bad Request ("invalid mount config for type "bind": bind source path does not exist: /tmp/airflowtmpennm3_n2")

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/docker/operators/docker.py", line 486, in execute
    return self._run_image()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/docker/operators/docker.py", line 357, in _run_image
    return self._run_image_with_mounts(self.mounts, add_tmp_variable=False)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/docker/operators/docker.py", line 421, in _run_image_with_mounts
    raise DockerContainerFailedException(f"Docker container failed: {result!r}", logs=log_lines)
airflow.providers.docker.exceptions.DockerContainerFailedException: Docker container failed: {'StatusCode': 1}
[2024-05-09T23:05:33.692+0000] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=pyspark_dag, task_id=pyspark_minio, execution_date=20240509T230058, start_date=20240509T230315, end_date=20240509T230533
[2024-05-09T23:05:33.704+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 64 for task pyspark_minio (Docker container failed: {'StatusCode': 1}; 7500)
[2024-05-09T23:05:33.728+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2024-05-09T23:05:33.741+0000] {taskinstance.py:2778} INFO - 0 downstream tasks scheduled from follow-on schedule check
